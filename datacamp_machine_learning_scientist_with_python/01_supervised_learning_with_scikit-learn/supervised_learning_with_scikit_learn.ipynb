{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning with Scikit-learn \n",
    "# 1. The Iris Flower Dataset\n",
    "\n",
    "In this notebook, I will apply the concepts learned in DataCamp to the famous Iris Flower Dataset collected by botanist E. S. Anderson and popularized by statistician and biologist Ronald Fisher.\n",
    "\n",
    "Track: Machine Learning Scientist With Python\n",
    "\n",
    "Course: Supervised Learning With Scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.utils.Bunch"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "iris = datasets.load_iris()\n",
    "type(iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Bunch is similar to a dictionary, in that it contains key-value pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "print(iris.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'DESCR' = Description of the dataset\n",
    "- 'feature_names' = a list with only four elements, which serve as column names for the four columns in 'data': 'petal_width', 'petal_length', 'sepal_width', 'sepal_length'\n",
    "- 'data' = ndarray with 4 columns, containing the numeric values for each of the feature_names.\n",
    "- 'target_names' = an array of just three values. Index 0 is 'setosa', 1 is 'versicolor' and 2 is 'virginica'.\n",
    "- 'target' = contains values 0, 1 or 2, indicating the corresponding 'target_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=21, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practice to perform the split so that your split reflects the labels on your data. That is, you want the labels to be distributed in the train and test sets as they are in the original dataset. To achieve this, we use the keyword argument stratify=y, where y is the list or array containing the labels.\n",
    "\n",
    "\n",
    "In order to implement K-Nearest Neighbors with scikit-learn, we need to take into account the following requirements:\n",
    "\n",
    "- Features must be continuous, not discrete.\n",
    "- There can't be missing values\n",
    "- Data has to be stored in numpy arrays or pandas dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use scikit-learn to fit a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 1\n",
      "Actual value: 2\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 2\n",
      "Actual value: 1\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 1\n",
      "Actual value: 1\n",
      "Prediction: 0\n",
      "Actual value: 0\n",
      "Prediction: 2\n",
      "Actual value: 2\n",
      "Prediction: 1\n",
      "Actual value: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# we instantiate the classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=6)\n",
    "\n",
    "# we fit the model to the data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# and now we use our model to label the test data\n",
    "prediction = knn.predict(X_test)\n",
    "\n",
    "for i,j in zip(prediction, y_test):\n",
    "    print(\"Prediction: {}\".format(i))\n",
    "    print(\"Actual value: {}\".format(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked pretty well! But, how well? How much can we trust our model?\n",
    "# Measuring performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of a model is defined as the fraction of correct predictions. This is included by scikit learn's score() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9555555555555556"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over and under fitting\n",
    "\n",
    "Using a small K number tends to result in overfitting (a more complex model that is too sensitive to noise in the specific data that you have, instead of reflecting general trends in the data), while a too large K will result in underfitting. These two images help understand how.\n",
    "\n",
    "<img src=\"figure1.png\">\n",
    "<img src=\"figure2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. The digits dataset\n",
    "\n",
    "Now let's try with more dimension. Scikit-learn includes a reduced version of the classic MNIST digits recognition dataset, which has 10 classes, the digits 0 through 9.\n",
    "\n",
    "Datacamp says:\n",
    "\n",
    "Each sample in this scikit-learn dataset is an 8x8 image representing a handwritten digit. Each pixel is represented by an integer in the range 0 to 16, indicating varying levels of black. Recall that scikit-learn's built-in datasets are of type Bunch, which are dictionary-like objects. Helpfully for the MNIST dataset, scikit-learn provides an 'images' key in addition to the 'data' and 'target' keys that you have seen with the Iris data. Because it is a 2D array of the images corresponding to each sample, this 'images' key is useful for visualizing the images, as you'll see in this exercise (for more on plotting 2D arrays, see Chapter 2 of DataCamp's course on Data Visualization with Python). On the other hand, the 'data' key contains the feature array - that is, the images as a flattened array of 64 pixels.\n",
    "\n",
    "Notice that you can access the keys of these Bunch objects in two different ways: By using the . notation, as in digits.images, or the [] notation, as in digits['images']. \n",
    "\n",
    "In the original MNIST dataset, the images are 28x28.\n",
    "\n",
    "First, we import and get to know our data\n",
    "\n",
    "## Exploring our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n",
      "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])\n",
      "(1797, 8, 8)\n",
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()\n",
    "\n",
    "print(digits.DESCR)\n",
    "print(digits.keys())\n",
    "\n",
    "print(digits.images.shape)\n",
    "print(digits.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print one of the numbers\n",
    "\n",
    "plt.imshow(digits.images[1010], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating arrays for the features and target variable, we will split them into training and test sets, fit a k-NN classifier to the training data, and then compute its accuracy using the .score() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9833333333333333\n"
     ]
    }
   ],
   "source": [
    "# Create feature and target arrays\n",
    "X_digits = digits.data\n",
    "y_digits = digits.target\n",
    "\n",
    "# Split into training and test set\n",
    "X_train_digits, X_test_digits, y_train_digits, y_test_digits = train_test_split(X_digits, y_digits, test_size = 0.2, random_state=42, stratify=y_digits)\n",
    "\n",
    "# Create a k-NN classifier with 7 neighbors: knn\n",
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "\n",
    "# Fit the classifier to the training data\n",
    "knn.fit(X_train_digits, y_train_digits)\n",
    "\n",
    "# Print the accuracy\n",
    "print(knn.score(X_test_digits, y_test_digits))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tailor-made models: finding the ideal fit\n",
    "\n",
    "Not too tight, not too loose, that's exactly how we want it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
